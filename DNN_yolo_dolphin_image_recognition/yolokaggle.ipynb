{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8623370,"sourceType":"datasetVersion","datasetId":5162402},{"sourceId":1418,"sourceType":"modelInstanceVersion","modelInstanceId":1199}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Kaggle Notebook: People Detection in Security Camera Footage using YOLOv5 (GPU Support)\n\nimport torch\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Check if GPU is available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Using device: {device}')\n\n# Load YOLOv5 model\nmodel = torch.hub.load('ultralytics/yolov5', 'yolov5s').to(device)\n\n# Define a function to process video frames\ndef process_frame(frame, model):\n    # Run YOLOv5 on the frame\n    results = model(frame)\n\n    # Extract bounding boxes and labels\n    boxes = results.xyxy[0].cpu().numpy()\n    labels = results.names\n    \n    return boxes, labels\n\n# Define a function to annotate frames with detected bounding boxes\ndef annotate_frame(frame, boxes, labels):\n    for box in boxes:\n        x1, y1, x2, y2, conf, cls = box\n        if labels[int(cls)] == 'person':\n            # Draw bounding box\n            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n            # Add label\n            cv2.putText(frame, f'{labels[int(cls)]} {conf:.2f}', (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    return frame\n\n# Load the video file\nvideo_path = '/kaggle/input/exp-01-jun-2024-1145-cam1-4-mp4/Exp_01_Jun_2024_1145_cam1-4.mp4'\ncap = cv2.VideoCapture(video_path)\n\n# Get video properties\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n# Define the codec and create VideoWriter object\noutput_path = '/kaggle/working/output_video.mp4'\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n# Check if video loaded successfully\nif not cap.isOpened():\n    print(\"Error opening video stream or file\")\n\n# Process the video frame by frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Split the frame into four quadrants\n    height, width, _ = frame.shape\n    half_height, half_width = height // 2, width // 2\n    \n    quadrants = [\n        frame[0:half_height, 0:half_width],\n        frame[0:half_height, half_width:width],\n        frame[half_height:height, 0:half_width],\n        frame[half_height:height, half_width:width]\n    ]\n    \n    # Process each quadrant\n    for i, quadrant in enumerate(quadrants):\n        # Ensure quadrant frame is in the correct format\n        quadrant_rgb = cv2.cvtColor(quadrant, cv2.COLOR_BGR2RGB)\n        boxes, labels = process_frame(quadrant_rgb, model)\n        quadrants[i] = annotate_frame(quadrant, boxes, labels)\n    \n    # Combine the quadrants back into a single frame\n    top_row = np.hstack((quadrants[0], quadrants[1]))\n    bottom_row = np.hstack((quadrants[2], quadrants[3]))\n    combined_frame = np.vstack((top_row, bottom_row))\n    \n    # Write the frame to the output video file\n    out.write(combined_frame)\n\n# Release video capture and writer objects\ncap.release()\nout.release()\n\nprint(\"Video processing complete. The output video is saved to /kaggle/working/output_video.mp4\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-06T13:31:40.182841Z","iopub.execute_input":"2024-06-06T13:31:40.183472Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\nYOLOv5 ðŸš€ 2024-6-6 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15102MiB)\n\nFusing layers... \nYOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\nAdding AutoShape... \n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}